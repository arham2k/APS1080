# -*- coding: utf-8 -*-
"""






# Part A Exercises

## 1, Explain clearly why V_pi is not useful in the MC development above?

Using Monte Carlo methods for policy improvement, the algorithm requires information about the states and the actions taken in those states. Hence, more than just the value function, which only gives the expected return in state ss, but not the actions, is required. According to MC methods, the algorithm requires action-value functions, which tell us the expected return for taking action aa in state ss and then following the current policy. With this function, the MC policy improvement method has enough information about the state-action pairs to make decisions or take action.

# 2, The MC algorithm so far (ref: p 99), requires an infinite number of episodes for Eval to converge on Q_pi_k (step k). We can modify this algorithm to the practical variant where Eval is truncated (c.f., DynProg GPI). In this case:

# a. Will we obtain Q_pi_k from eval?

We will not get the state value function from a truncated MC algorithm as we are simply not letting the algorithm gather enough episodes to accurately determine all the action values for all state-action pairs.

# b. If not why are we able to truncate Eval? Explain clearly.'

We can alternate between policy improvement and policy evaluation via GPI for each step, and thus we can truncate the overall evaluation methodology in MC as long as the number of episodes is sufficient. By having enough samples, the estimates will eventually converge to the optimal Q, or at least close to it. If we ensure enough exploration in the algorithm, where all the actions of the state-action pairs are visited, the estimate will be useful to update the state-action value function. While it may not converge to the optimal Q due to the exploration, it will get close enough for real-life use.

# c. Assuming exhaustive sampling (ES) and the above truncated Eval_trunc​, is it possible to converge on a sub-optimal policy π_c? Is this a stable fixed point of the GPI for MC? Explain clearly.


It can converge to a sub-optimal policy when using the truncated evaluation because the number of episodes is limited, the evaluation algorithm is not completely accurate, and hence the optimal policy it eventually converges to may not be reached. It will also be a stable fixed point for the GPI process in terms of MC if the evaluation of the sub-optimal policy remains consistent, i.e., no more improvements are being observed. As long as the exploration is sufficient, the algorithm will stabilize at this sub-optimal policy due to no further improvements being seen.

# 3. Explain how you can synthesize a stochastic policy given what you know so far

It can converge to a sub-optimal policy when using the truncated evaluation because the number of episodes is limited, the evaluation algorithm is not completely accurate, and hence the optimal policy it eventually converges to may not be reached. It will also be a stable fixed point for the GPI process in terms of MC if the evaluation of the sub-optimal policy remains consistent, i.e., no more improvements are being observed. As long as the exploration is sufficient, the algorithm will stabilize at this sub-optimal policy due to no further improvements being seen.

# Part B Excercises


## 1, Code the algorithm for MC Control (Off Policy) and apply this to the Cart Pole problem
"""

!apt-get install -y xvfb python-opengl > /dev/null 2>&1
!apt-get update

!pip install gym pyvirtualdisplay > /dev/null 2>&1
!apt-get install -y xvfb

import gym
import numpy as np
import matplotlib.pyplot as plt
from IPython import display as ipythondisplay

from pyvirtualdisplay import Display
display = Display(visible=0, size=(400, 300))
display.start()

# Sample Code
env = gym.make("CartPole-v0")
env.reset()
prev_screen = env.render(mode='rgb_array')
plt.imshow(prev_screen)

for i in range(50000):
  action = env.action_space.sample()
  print("step i",i,"action=",action)
  obs, reward, done, info = env.step(action)
  print("obs=",obs,"reward=",reward,"done=",done,"info=",info)
  screen = env.render(mode='rgb_array')

  plt.imshow(screen)
  ipythondisplay.clear_output(wait=True)
  ipythondisplay.display(plt.gcf())

  if done:
    break

ipythondisplay.clear_output(wait=True)
env.close()
print("Iterations that were run:",i)

def discretize_state(observation, bins):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    state = (
        np.clip(np.digitize(cart_pos, bins[0]), 0, len(bins[0])-1), np.clip(np.digitize(cart_vel, bins[1]), 0, len(bins[1])-1), np.clip(np.digitize(pole_angle, bins[2]), 0, len(bins[2])-1),
        np.clip(np.digitize(pole_vel, bins[3]), 0, len(bins[3])-1)
    )
    return state

env = gym.make("CartPole-v0")

bins = [np.linspace(-2.4, 2.4, 10),  np.linspace(-3.0, 3.0, 10),  np.linspace(-0.5, 0.5, 10), np.linspace(-2.0, 2.0, 10)]

Q = np.random.rand(10, 10, 10, 10, env.action_space.n)
C = np.zeros((10, 10, 10, 10, env.action_space.n))
greedy_policy = np.zeros((10, 10, 10, 10), dtype=int)

gamma = 0.99
epsilon = 0.05
max_steps = 10000
left = 0
right = 0

for episode in range(max_steps):
    observation = env.reset()
    state = discretize_state(observation, bins)
    episode_data = []
    done = False

    while not done:
        if np.random.rand() < epsilon:
            action = np.random.choice([0, 1])
        else:
            action = greedy_policy[state]

        new_obs, reward, done, _ = env.step(action)
        new_state = discretize_state(new_obs, bins)
        episode_data.append((state, action, reward))
        state = new_state

        #screen = env.render(mode='rgb_array')
        #plt.imshow(screen)
        #ipythondisplay.clear_output(wait=True)
        #ipythondisplay.display(plt.gcf())

    G = 0
    W = 1

    for t in reversed(range(len(episode_data))):
        state, action, reward = episode_data[t]
        G = gamma * G + reward
        C[state][action] += W
        Q[state][action] += W / C[state][action] * (G - Q[state][action])
        greedy_policy[state] = np.argmax(Q[state])


        if action != greedy_policy[state]:
            break

        W *= 1 / (epsilon / env.action_space.n + (1 - epsilon) * (action == greedy_policy[state]))

    if episode % 100 == 0:
        print(f"The pole has been balanced for {episode} episodes  or {len(episode_data)}, time steps")
        if action == 0:
            print("The action taken at this time step is Left")
            left = left + 1
        else:
            print("The action taken at this time step is Right")
            right = right + 1
        episodes_left = max_steps-len(episode_data)
        print(f"There are still {episodes_left} steps left in this episode\n")

        if episode >= 1000:
          print(f"The Pole went Left {left}, times and Right {right}, times in 10000 episodes")
          break

env.close()

