# -*- coding: utf-8 -*-
"""ArhamSheikh_APS1080_Exercise2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1glDkQU20-xPPG8wBYfmfb-_7qxjb-gre

# Arham Sheikh APS 1080 Exercise 2

# Exercise 1: Can you design a dynamic programming based policy for the agent as in assignment 1? If so, design it and demonstrate that it solves the cart pole problem.

We can design a dynamic programming based policy for the agent for the Cart Pole problem however it would not work since we do not have the entire model of the agent. In assignment 1, the environment was a grid-like environment where all the states and their transition probabilities are known, however in this problem, two actions are possible i.e. Left or Right. This only gives us the actions but the states this puts the agent, if we do not have the entire model of the environment mapped out for our Dynamic Programing based policy then the policy since we do not have the transition probabilities nor do we have a set reward function that tells us how favourable the Left/Right actions are.

# Exercise 2: Can you design a Monte Carlo based policy for the agent? What ingredients do you require? Explain the design flow, and execute it. Show that it works, or indicate why you can't proceed.

It is possible to design a MC based policy for the agent, the ingredients we need to do this are a function that will discretize the continuous states into discrete states that we can observe, and then we need to have a random initialized Q function that will arbitrarily choose an action in the beginning. According to the algorithm on page 111 for the Off Policy MC method we will also require a counter that will track the number of visits and a greedy and behaviour policy. The design flow of the MC method as shown in the pseudocode referred to in the section above follows as

# Design Flow
1. Initialization
* Environment Setup
* Initialize random Q function
* Intilize counter function
* Initialize greedy policy function
2. Action Selection
* Action is randomly chosen from random arbitrary policy, and could either exploit or explore the environment
* Discretize the Environments initial observation
3. Environment Step
* New observations are taken from the environment and discretize them.
4. Repeat until the episode ends
5. Return is calculated
* Calculate G from the return function equation
6. Update Q function
* Update Q and C functions with accumulated returns
7. Importance Weight Sampling
"""

import numpy as np
import matplotlib.pyplot as plt
import gym


def discretize_state(observation, bins):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    state = ( np.clip(np.digitize(cart_pos, bins[0]), 0, len(bins[0])-1),np.clip(np.digitize(cart_vel, bins[1]), 0, len(bins[1])-1),
        np.clip(np.digitize(pole_angle, bins[2]), 0, len(bins[2])-1), np.clip(np.digitize(pole_vel, bins[3]), 0, len(bins[3])-1))
    return state


env = gym.make("CartPole-v0")
bins = [np.linspace(-2.4, 2.4, 10), np.linspace(-3.0, 3.0, 10),np.linspace(-0.5, 0.5, 10),np.linspace(-2.0, 2.0, 10)]

Q = np.random.rand(10, 10, 10, 10, env.action_space.n)
C = np.zeros((10, 10, 10, 10, env.action_space.n))
greedy_policy = np.zeros((10, 10, 10, 10), dtype=int)


gamma = 0.99
epsilon = 0.1
max_eps = 0
episode_rewards = []


while max_eps < 500:
    observation = env.reset()
    state = discretize_state(observation, bins)
    episode_data = []
    total_reward = 0
    done = False

    while not done:

        if np.random.rand() < epsilon:
            action = np.random.choice([0, 1])
        else:
            action = greedy_policy[state]

        new_obs, reward, done, _ = env.step(action)
        total_reward += reward
        new_state = discretize_state(new_obs, bins)
        episode_data.append((state, action, reward))
        state = new_state

    episode_rewards.append(total_reward)

    G = 0
    W = 1

    for t in reversed(range(len(episode_data))):
        state, action, reward = episode_data[t]
        G = gamma * G + reward
        C[state][action] += W
        Q[state][action] += W / C[state][action] * (G - Q[state][action])
        greedy_policy[state] = np.argmax(Q[state])

        if action != greedy_policy[state]:
            break


        W *= 1 / (epsilon / env.action_space.n + (1 - epsilon) * (action == greedy_policy[state]))
    max_eps += 1

env.close()

plt.figure(figsize=(12, 6))
plt.plot(episode_rewards)
plt.xticks(ticks=np.arange(0, max_eps + 1, 50))
plt.grid()
plt.title("Rewards Per Episode")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.show()

print(f"\nThe episode with the most rewards is Episode {np.argmax(episode_rewards)} with a reward of {episode_rewards[np.argmax(episode_rewards)]}")