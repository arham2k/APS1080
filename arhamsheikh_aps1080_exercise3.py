# -*- coding: utf-8 -*-
"""ArhamSheikh_APS1080_Exercise3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uv28nKjiu5-sOGGfyMphwD9EmMM8wgtP

# Arham Sheikh APS 1080 Exercise 3

# A,  On-Policy SARSA
"""

import gym
import numpy as np
import matplotlib.pyplot as plt


# Create the CartPole environment
env = gym.make('CartPole-v0')

alpha = 0.1
epsilon = 0.1
gamma = 0.99
episode_rewards = []
episode = 0

def discretize_state(observation, bins):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    state = (np.clip(np.digitize(cart_pos, bins[0]), 0, len(bins[0])-1), np.clip(np.digitize(cart_vel, bins[1]), 0, len(bins[1])-1),
        np.clip(np.digitize(pole_angle, bins[2]), 0, len(bins[2])-1), np.clip(np.digitize(pole_vel, bins[3]), 0, len(bins[3])-1) )
    return state


bins = [np.linspace(-4.8, 4.8, 10), np.linspace(-3.0, 3.0, 10),  np.linspace(-0.418, 0.418, 10), np.linspace(-2.0, 2.0, 10)]


Q = np.random.rand(10, 10, 10, 10, env.action_space.n)

# SARSA
while (episode < 1000):

  observation = env.reset()
  state = discretize_state(observation, bins)
  total_reward = 0
  done = False

  if np.random.uniform(0, 1) < epsilon:
    action = np.random.choice(env.action_space.n)
  else:
    action =  np.argmax(Q[state])


  while not done:
    new_obs, reward, done, _ = env.step(action)
    next_state = discretize_state(new_obs, bins)

    if np.random.uniform(0, 1) < epsilon:
      next_action = np.random.choice(env.action_space.n)
    else:
      next_action =  np.argmax(Q[next_state])

    Q[state][action] += alpha * (reward + gamma * Q[next_state][next_action] - Q[state][action])

    state, action = next_state, next_action
    total_reward += reward
   # print(episode_rewards)

  episode += 1
  episode_rewards.append(total_reward)

  if episode % 100 == 0:
    print(f"Episode {episode} completed with a reward of {total_reward}")



plt.figure(figsize=(12, 6))
plt.plot(episode_rewards)
plt.xticks(ticks=np.arange(0, max_eps + 1, 50))
plt.grid()
plt.title("Rewards Per Episode")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.show()

print(f"\nThe episode with the most rewards is Episode {np.argmax(episode_rewards)} with a reward of {episode_rewards[np.argmax(episode_rewards)]}")

env.close()

"""# B, Off-Policy Q Learning"""

import gym
import numpy as np
import matplotlib.pyplot as plt

env = gym.make('CartPole-v0')


alpha = 0.1
epsilon = 0.1
gamma = 0.99
max_eps = 1000
episode_rewards = []
episode = 0

def discretize_state(observation, bins):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    state = (np.clip(np.digitize(cart_pos, bins[0]), 0, len(bins[0])-1), np.clip(np.digitize(cart_vel, bins[1]), 0, len(bins[1])-1),
        np.clip(np.digitize(pole_angle, bins[2]), 0, len(bins[2])-1), np.clip(np.digitize(pole_vel, bins[3]), 0, len(bins[3])-1) )
    return state


bins = [np.linspace(-4.8, 4.8, 10), np.linspace(-3.0, 3.0, 10),  np.linspace(-0.418, 0.418, 10), np.linspace(-2.0, 2.0, 10)]


Q = np.random.rand(10, 10, 10, 10, env.action_space.n)


# Q-learning
while (episode < max_eps):
  observation = env.reset()
  state = discretize_state(observation, bins)
  done = False
  total_reward = 0

  while not done:
    if np.random.uniform(0, 1) < epsilon:
      action = np.random.choice(env.action_space.n)
    else:
      action = np.argmax(Q[state])

    next_obs, reward, done, _ = env.step(action)
    next_state = discretize_state(next_obs, bins)

    Q[state][action] += alpha * (reward + gamma * np.max(Q[next_state]) - Q[state][action])
    state = next_state

    total_reward += reward
  # print(reward)

  episode += 1
  episode_rewards.append(total_reward)

  if episode % 100 == 0:
    print(f"Episode {episode} completed with a reward of {total_reward}")



plt.figure(figsize=(12, 6))
plt.plot(episode_rewards)
plt.xticks(ticks=np.arange(0, max_eps + 1, 50))
plt.grid()
plt.title("Rewards Per Episode")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.show()


print(f"\nThe episode with the most rewards is Episode {np.argmax(episode_rewards)} with a reward of {episode_rewards[np.argmax(episode_rewards)]}")

env.close()

"""# C, Off Policy Expected SARSA with Epilson Greedy Policy

"""

import gym
import numpy as np
import matplotlib.pyplot as plt


env = gym.make('CartPole-v0')


alpha = 0.1
epsilon = 0.1
gamma = 0.99
max_eps = 1000
episode_rewards = []
episode = 0

def discretize_state(observation, bins):
    cart_pos, cart_vel, pole_angle, pole_vel = observation
    state = (np.clip(np.digitize(cart_pos, bins[0]), 0, len(bins[0])-1), np.clip(np.digitize(cart_vel, bins[1]), 0, len(bins[1])-1),
        np.clip(np.digitize(pole_angle, bins[2]), 0, len(bins[2])-1), np.clip(np.digitize(pole_vel, bins[3]), 0, len(bins[3])-1) )
    return state


bins = [np.linspace(-4.8, 4.8, 10), np.linspace(-3.0, 3.0, 10),  np.linspace(-0.418, 0.418, 10), np.linspace(-2.0, 2.0, 10)]

Q = np.random.rand(10, 10, 10, 10, env.action_space.n)

# Expected SARSA
while (episode < max_eps):
  observation = env.reset()
  state = discretize_state(observation, bins)

  done = False
  total_reward = 0
  length = 0

  while not done:
    if np.random.uniform(0, 1) < epsilon:
      action = np.random.choice(env.action_space.n)
    else:
      action = np.argmax(Q[state])

    next_obs, reward, done, _ = env.step(action)
    next_state = discretize_state(next_obs, bins)

    new_Q = 0

    for best_action in range(env.action_space.n):
      if best_action == np.argmax(Q[next_state]):
        new_Q += (1 - epsilon) * Q[next_state][best_action]
      else:
        new_Q += (epsilon / env.action_space.n) * Q[next_state][best_action]


    Q[state][action] += alpha * (reward + gamma * new_Q - Q[state][action])
    state = next_state

    total_reward += reward

  episode += 1

  episode_rewards.append(total_reward)

  if episode % 100 == 0:
    print(f"Episode {episode} completed with a reward of {total_reward}")


plt.figure(figsize=(12, 6))
plt.plot(episode_rewards)
plt.xticks(ticks=np.arange(0, max_eps + 1, 50))
plt.grid()
plt.title("Rewards Per Episode")
plt.xlabel("Episode")
plt.ylabel("Reward")
plt.show()

print(f"\nThe episode with the most rewards is Episode {np.argmax(episode_rewards)} with a reward of {episode_rewards[np.argmax(episode_rewards)]}")
env.close()